{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TP4: ANNs: The Perceptron Model\n",
        "## Breast cancer prediction\n",
        "We will see how to implement the perceptron model using breast cancer data set in python.\n",
        "\n",
        "A perceptron is a fundamental unit of the neural network which takes weighted inputs, process it and capable of performing binary classifications.\n",
        "\n",
        "The data set we will be using is breast cancer data set from sklearn. The data set has 569 observations and 30 variables excluding the class variable.  "
      ],
      "metadata": {
        "id": "NshEJ3Q5izUL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwMkWSHEixUd"
      },
      "outputs": [],
      "source": [
        "import sklearn.datasets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Python code imports necessary modules and packages for data analysis and machine learning tasks:\n",
        "\n",
        "`sklearn.datasets` module: This is a module from the Scikit-Learn library that provides datasets for machine learning tasks. It is used to import datasets to be used in the analysis.\n",
        "`numpy package`: This package provides numerical computing tools in Python. It is imported with the `np` alias.\n",
        "`pandas package`: This is a package for data manipulation and analysis. It is imported with the pd alias.\n",
        "`matplotlib.pyplot` module: This is a module from the Matplotlib library used for data visualization in Python. It is imported with the `plt` alias and `%matplotlib inline` is used to display plots in the  notebook.\n",
        "`sklearn.model_selection` module: This is a module from Scikit-Learn library used for model selection and evaluation. It is used to split the dataset into training and testing sets."
      ],
      "metadata": {
        "id": "J7rMjvwNiyLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the breast cancer data\n",
        "breast_cancer = sklearn.datasets.load_breast_cancer()"
      ],
      "metadata": {
        "id": "en2qp1DnjvZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line of code loads the breast cancer dataset from Scikit-Learn's datasets module using the `load_breast_cancer` function.\n",
        "\n",
        "The `breast_cancer` variable is assigned to the dataset object which contains both the data and the target labels. The data consists of features that describe characteristics of breast mass cells and the target labels indicate whether the mass is benign or malignant."
      ],
      "metadata": {
        "id": "Rm0oGNWsj6PJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the data to pandas dataframe\n",
        "data = pd.DataFrame(breast_cancer.data, columns = breast_cancer.feature_names)\n",
        "data['class'] = breast_cancer.target\n",
        "print(breast_cancer.target_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4asOVgZj7_W",
        "outputId": "da3395ee-8114-45dc-bb20-5a0ecd45df4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['malignant' 'benign']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates a pandas DataFrame called `data` from the breast cancer dataset loaded in the previous line.\n",
        "\n",
        "The `pd.DataFrame()` function takes in two arguments:\n",
        "\n",
        "* `breast_cancer.data`: this is the data from the breast cancer dataset, which is a numpy array containing the features.\n",
        "\n",
        "* `columns = breast_cancer.feature_names`: this sets the column names for the DataFrame to be the feature names from the dataset.\n",
        "\n",
        "The second line of code adds a new column to the data DataFrame called `'class'`. The `breast_cancer.target` array contains the target labels for each sample in the dataset, where `0` indicates a benign tumor and `1` indicates a malignant tumor. By assigning the `breast_cancer.target` array to the `'class'` column, the DataFrame now contains both the features and target labels for each sample."
      ],
      "metadata": {
        "id": "kmwDQfAxkKVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting a graph to see class counts\n",
        "data['class'].value_counts().plot(kind = \"barh\")\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Classes\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2lPSWI2fkYux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code visualizes the distribution of target classes in the breast cancer dataset using a horizontal bar chart.\n",
        "\n",
        "The `value_counts()` method is applied to the `'class'` column of the data DataFrame to count the number of samples for each class. The resulting counts are then plotted using the `plot()` method with `kind = \"barh\"` argument, which specifies a horizontal bar chart.\n",
        "\n",
        "The `plt.xlabel()` and` plt.ylabel()` functions are used to set the labels for the x-axis and y-axis, respectively.\n",
        "\n",
        "Finally, `plt.show()` is used to display the plot in the  notebook."
      ],
      "metadata": {
        "id": "iJIX3xaDkgHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.drop(\"class\", axis = 1)\n",
        "Y = data['class']\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, stratify = Y, random_state=1)"
      ],
      "metadata": {
        "id": "I6RB75CekoSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code prepares the breast cancer dataset for training and testing a machine learning model.\n",
        "\n",
        "The `X` variable is assigned the data DataFrame with the `'class'` column dropped using the `drop()` method along the `axis = 1` argument. This creates a new DataFrame containing only the features.\n",
        "\n",
        "The `Y` variable is assigned the `'class'` column of the data DataFrame, which contains the target labels.\n",
        "\n",
        "The `train_test_split()` function from Scikit-Learn's model_selection module is used to split the dataset into training and testing sets. The function takes the following arguments:\n",
        "\n",
        "* `X`: the features DataFrame\n",
        "* `Y`: the target labels Series\n",
        "* `test_size = 0.1`: the proportion of the dataset to use for testing (in this case, 10% of the dataset)\n",
        "* `stratify = Y`: this argument ensures that the target classes are evenly distributed in both the training and testing sets\n",
        "* `random_state = 1`: this argument sets a random seed for reproducibility\n",
        "\n",
        "The function returns four sets of data:\n",
        "\n",
        "`X_train`: the features for the training set\n",
        "`X_test`: the features for the testing set\n",
        "`Y_train`: the target labels for the training set\n",
        "`Y_test`: the target labels for the testing set\n",
        "\n",
        "Overall, this code prepares the breast cancer dataset for training and testing a machine learning model, with 90% of the dataset used for training and 10% used for testing."
      ],
      "metadata": {
        "id": "Kr84tkuUk3ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(X_test.T, '*')\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DxI0lH1vk43E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates a scatter plot for the testing set features in the breast cancer dataset.\n",
        "\n",
        "The `plt.plot()` function is used to plot the features in the testing set, with the `'*'` argument specifying the marker style. The `.T` attribute is used to transpose the testing set DataFrame so that each feature is plotted on its own row.\n",
        "\n",
        "The `plt.xticks()` function is used to set the x-axis labels to be vertical using the `rotation='vertical'` argument.\n",
        "\n",
        "Finally, `plt.show()` is used to display the plot in the  notebook.\n",
        "\n",
        "The resulting plot shows a scatter plot for each feature in the testing set, with each point representing a sample in the testing set. The plot provides a visual representation of the distribution of the testing set features, which can be useful for identifying patterns and relationships between the features."
      ],
      "metadata": {
        "id": "GZnDCdWkusH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "osyvnjvV4A-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code imports the `StandardScaler` class from the `sklearn.preprocessing` module and creates a `StandardScale`r object called `scaler`.\n",
        "\n",
        "The `fit_transform` method is called on the `scaler` object once for the training data `X_train`. This method computes the mean and standard deviation of each feature in the training data and then scales the features so that they have a mean of zero and a standard deviation of one.\n",
        "\n",
        "The `transform` method is then called on the `scaler` object for the test data `X_test`. This method uses the same mean and standard deviation values that were computed during the `fit_transform` step on the training data to scale the test data in the same way.\n",
        "\n",
        "It's important to note that the `fit_transform` method should only be called on the training data. This is because the scaler is \"fit\" to the training data to compute the mean and standard deviation values, which are then used to scale both the training and test data. If the `fit_transform` method were called on the test data, it would be using mean and standard deviation values that are specific to the test data and may not be representative of the entire dataset.\n",
        "\n",
        "By calling `transform` on the test data using the `scaler` that was fit to the training data, we ensure that the test data is scaled in the same way as the training data. This is important for accurate predictions from a machine learning model, since the model has been trained on the scaled data and therefore expects new data to be similarly scaled."
      ],
      "metadata": {
        "id": "DJv4opB54Xs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class Perceptron:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.b = None\n",
        "        self.w = None\n",
        "\n",
        "    def model(self, x):\n",
        "        return 1 if np.dot(self.w, x) >= self.b else 0\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = []\n",
        "        for x in X:\n",
        "            result = self.model(x)\n",
        "            y_pred.append(result)\n",
        "        return np.array(y_pred)\n",
        "\n",
        "    def fit(self, X, Y, epochs = 1, lr = 1):\n",
        "        self.b = 0\n",
        "        self.w = np.zeros(X.shape[1])\n",
        "        accuracy = {}\n",
        "        max_accuracy = 0\n",
        "\n",
        "        for i in range(epochs):\n",
        "            for x, y in zip(X, Y):\n",
        "                y_pred = self.model(x)\n",
        "                if y == 1 and y_pred == 0:\n",
        "                    self.w += lr * x\n",
        "                    self.b += lr * 1\n",
        "                elif y == 0 and y_pred == 1:\n",
        "                    self.w -= lr * x\n",
        "                    self.b -= lr * 1\n",
        "\n",
        "            accuracy[i] = accuracy_score(self.predict(X), Y)\n",
        "            if (accuracy[i] > max_accuracy):\n",
        "                max_accuracy = accuracy[i]\n",
        "                checkpoint_w = self.w\n",
        "                checkpoint_b = self.b\n",
        "\n",
        "        self.w = checkpoint_w\n",
        "        self.b = checkpoint_b\n",
        "\n",
        "        print(\"Max Accuracy: \", max_accuracy)\n",
        "        plt.plot(list(accuracy.values()))\n",
        "        plt.ylim([0, 1])\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "Hmduh_j34URg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a `Perceptron` class that includes methods for defining the perceptron model, making predictions, and fitting the model to the training data. The `fit()` method implements the perceptron learning algorithm to update the weights and bias of the model based on the training data, with options to specify the number of epochs and learning rate."
      ],
      "metadata": {
        "id": "7KCSvkT05J71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perceptron1 = Perceptron()\n",
        "perceptron1.fit(X_train_scaled, Y_train, epochs=1000, lr=0.01)"
      ],
      "metadata": {
        "id": "Nfrrn6Fi5bhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates a `Perceptron` object called `perceptron1`. The `fit` method is then called on this object with four arguments: `X_train_scaled`, `Y_train`, `epochs=1000`, and `lr=0.01`.\n",
        "\n",
        "`X_train_scaled` is the training data that has been rescaled using a StandardScaler object. `Y_train` is the target variable associated with the training data. `epochs=1000` specifies the number of times that the algorithm should iterate over the entire training dataset during training. `lr=0.01` is the learning rate, which determines the step size that the algorithm takes during each iteration.\n",
        "\n",
        "When the `fit` method is called, the weights and bias of the perceptron are initialized to small random values. Then, the algorithm loops through the training data for the specified number of epochs, making predictions for each sample and updating the weights and bias based on the prediction error. The algorithm continues to make predictions and update the weights until either the specified number of epochs is reached or the prediction error falls below a certain threshold.\n",
        "\n",
        "The end result of fitting the perceptron to the training data is a set of weights and a bias value that can be used to make predictions for new, unseen data."
      ],
      "metadata": {
        "id": "Q21uOeAYI1Qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred_test = perceptron1.predict(X_test_scaled)\n",
        "print(accuracy_score(Y_pred_test, Y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zs7OS2o3I13-",
        "outputId": "6dc49417-870a-4cbe-9ed0-1d4870b0e611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training the perceptron on the scaled training data, the code above uses the trained perceptron to make predictions on the scaled test data using the `predict` method of the `perceptron1` object. The predicted labels are stored in `Y_pred_test`.\n",
        "\n",
        "Finally, the `accuracy_score` function from the `sklearn.metrics` module is used to calculate the accuracy of the predictions. `Y_test` contains the true labels for the test data. The `accuracy_score` function compares the predicted labels with the true labels and returns the fraction of correctly classified samples. This accuracy score is then printed to the console.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "72TMen9RI2gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = perceptron1.w\n",
        "b = perceptron1.b\n",
        "print(\"Weights:\", w)\n",
        "print(\"Bias:\", b)"
      ],
      "metadata": {
        "id": "LI7Rish0I3Ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training the perceptron, the code above retrieves the learned weights and bias of the perceptron model. The learned weights are stored in the `w` attribute of the `perceptron1` object, and the learned bias is stored in the `b` attribute.\n",
        "\n",
        "The code then prints the learned weights and bias to the console using print statements."
      ],
      "metadata": {
        "id": "PXkDmoDkCUtQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To Do:\n",
        "\n",
        "## Task 1\n",
        "Experiment with different dataset split ratios and different hyperparameters of the Perceptron model, i.e.,  the learning rate and the number of iterations, and observe how they affect the performance of the model.\n",
        "\n",
        "## Task 2\n",
        "Perform a similar perceptron learning excercise on the banknote dataset to predict whether a banknote is fraudulant or not."
      ],
      "metadata": {
        "id": "rliHsy8wInAA"
      }
    }
  ]
}